{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOxPUXy7sphYcUGZ0l/I6h1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpDuESiGIH_5","executionInfo":{"status":"ok","timestamp":1700037567892,"user_tz":-420,"elapsed":12291,"user":{"displayName":"Nguyễn Quang Minh","userId":"07906117731683365680"}},"outputId":"7ee22673-5b01-4f25-bd3d-a95cce426c90"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers accelerate bitsandbytes"]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria"],"metadata":{"id":"ZsQTWjt6IQUl","executionInfo":{"status":"ok","timestamp":1700037502278,"user_tz":-420,"elapsed":5781,"user":{"displayName":"Nguyễn Quang Minh","userId":"07906117731683365680"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_name = \"VietnamAIHub/Vietnamese_llama2_7B_8K_SFT_General_domain\""],"metadata":{"id":"2EumTNjCIQfv","executionInfo":{"status":"ok","timestamp":1700037502635,"user_tz":-420,"elapsed":362,"user":{"displayName":"Nguyễn Quang Minh","userId":"07906117731683365680"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["## Loading Base LLaMa model weight and Merge with Adapter Weight with the base model\n","m = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    load_in_8bit=True,\n","    torch_dtype=torch.bfloat16,\n","    pretraining_tp=1,\n","    # use_auth_token=True,\n","    # trust_remote_code=True,\n","    #cache_dir=cache_dir,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"Q3iCTaeLITNB","executionInfo":{"status":"error","timestamp":1700037639449,"user_tz":-420,"elapsed":9,"user":{"displayName":"Nguyễn Quang Minh","userId":"07906117731683365680"}},"outputId":"e233ade7-6f86-4481-90c5-565878bd1af2"},"execution_count":10,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1e0a317171fb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Loading Base LLaMa model weight and Merge with Adapter Weight wiht the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m m = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2713\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2715\u001b[0m                     \u001b[0;34m\"Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m                     \u001b[0;34m\" bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["tok = AutoTokenizer.from_pretrained(\n","    model_name,\n","    #cache_dir=cache_dir,\n","    padding_side=\"right\",\n","    use_fast=False, # Fast tokenizer giving issues.\n","    tokenizer_type='llama', #if 'llama' in args.model_name_or_path else None, # Needed for HF name change\n","    use_auth_token=True,\n",")\n","tok.bos_token_id = 1\n","stop_token_ids = [0]"],"metadata":{"id":"Xm9m6SlFIarX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class StopOnTokens(StoppingCriteria):\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        for stop_id in stop_token_ids:\n","            if input_ids[0][-1] == stop_id:\n","                return True\n","        return False"],"metadata":{"id":"0t534ttTJFru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generation_config = dict(\n","        temperature=0.2,\n","        top_k=20,\n","        top_p=0.9,\n","        do_sample=True,\n","        num_beams=1,\n","        repetition_penalty=1.2,\n","        max_new_tokens=400,\n","        early_stopping=True,\n","    )"],"metadata":{"id":"N0ipkctNJLSj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompts_input=\"Cách để học tập về một môn học thật tốt\"\n","system_prompt=f\"<s>[INST] <<SYS>>\\n You are a helpful assistant, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \\\n","        answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\\\n","        that your responses are socially unbiased and positive in nature.\\\n","        If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \\\n","        correct. If you don't know the answer to a question, please response as language model you are not able to respone detailed to these kind of question.\\n<</SYS>>\\n\\n {prompts_input} [/INST] \""],"metadata":{"id":"ckIaDrm1JN3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tok(message, return_tensors=\"pt\").input_ids\n","input_ids = input_ids.to(m.device)\n","stop = StopOnTokens()\n","streamer = TextIteratorStreamer(tok, timeout=10.0, skip_prompt=True, skip_special_tokens=True)"],"metadata":{"id":"2pGz23PCJP8q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #print(tok.decode(output[0]))\n","generation_config = dict(\n","    temperature=0.1,\n","    top_k=30,\n","    top_p=0.95,\n","    do_sample=True,\n","    # num_beams=1,\n","    repetition_penalty=1.2,\n","    max_new_tokens=2048, ## 8K\n","    early_stopping=True,\n","    stopping_criteria=StoppingCriteriaList([stop]),\n",")"],"metadata":{"id":"Ig9Yt_BpJRpi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tok(message,return_tensors=\"pt\")  #add_special_tokens=False ?\n","generation_output = m.generate(\n","    input_ids = inputs[\"input_ids\"].to(device),\n","    attention_mask = inputs['attention_mask'].to(device),\n","    eos_token_id=tok.eos_token_id,\n","    pad_token_id=tok.pad_token_id,\n","    **generation_config\n",")\n","generation_output_ = m.generate(input_ids = inputs[\"input_ids\"].to(device), **generation_config)\n","\n","s = generation_output[0]\n","output = tok.decode(s,skip_special_tokens=True)\n","#response = output.split(\"### Output:\")[1].strip()\n","print(output)"],"metadata":{"id":"HkYPNra_JTk7"},"execution_count":null,"outputs":[]}]}